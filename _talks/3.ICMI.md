---
title: "Conference talk on Gesture Enhanced Comprehension of Ambiguous Human-to-Robot Instructions"
collection: talks
type: "Conference proceedings talk"
permalink: /talks/3.ICMI
venue: "International Conference on Multimodal Interaction (ICMI 2020)"
date: 2020-10-27
location: "Remote"
---

[Download talk](https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3382507.3418863&file=3382507.3418863.mp4)

This work demonstrates the feasibility and benefits of using pointing gestures, a naturally-generated additional input modality, to improve the multi-modal comprehension accuracy of human instructions to robotic agents for collaborative tasks.We present M2Gestic, a system that combines neural-based text parsing with a novel knowledge-graph traversal mechanism, over a multi-modal input of vision, natural language text and pointing. Via multiple studies related to a benchmark table top manipulation task, we show that (a) M2Gestic can achieve close-to-human performance in reasoning over unambiguous verbal instructions, and (b) incorporating pointing input (even with its inherent location uncertainty) in M2Gestic results in a significant (30%) accuracy improvement when verbal instructions are ambiguous.
