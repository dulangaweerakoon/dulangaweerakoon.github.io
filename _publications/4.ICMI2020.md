---
title: "Gesture enhanced comprehension of ambiguous human-to-robot instructions"
collection: publications
permalink: /publication/icmi
excerpt: ''
date: 21.10.2020
venue: 'Proceedings of the 2020 International Conference on Multimodal Interaction'
paperurl: 
citation: 'Weerakoon, D., Subbaraju, V., Karumpulli, N., Tran, T., Xu, Q., Tan, U.X., Lim, J.H. and Misra, A., 2020, October. Gesture enhanced comprehension of ambiguous human-to-robot instructions. In Proceedings of the 2020 International Conference on Multimodal Interaction (pp. 251-259).'
---
This work demonstrates the feasibility and benefits of using pointing gestures, a naturally-generated additional input modality, to improve the multi-modal comprehension accuracy of human instructions to robotic agents for collaborative tasks. We present M2Gestic, a system that combines neural-based text parsing with a
novel knowledge-graph traversal mechanism, over a multi-modal input of vision, natural language text and pointing. Via multiple studies related to a benchmark table top manipulation task, we show that (a) M2Gestic can achieve close-to-human performance in reasoning over unambiguous verbal instructions, and (b) incorporating pointing input (even with its inherent location uncertainty) in M2Gestic results in a significant (âˆ¼ 30%) accuracy improvement when verbal instructions are ambiguous.

[Download paper here](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=6373&context=sis_research)

<!-- Recommended citation: Weerakoon, D., Subbaraju, V., Tran, T. and Misra, A., 2022. Cosm2ic: Optimizing real-time multi-modal instruction comprehension. IEEE Robotics and Automation Letters, 7(4), pp.10697-10704. -->