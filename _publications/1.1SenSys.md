---
title: "VGGlass - Demonstrating Visual Grounding and Localization Synergy with a LiDAR-enabled Smart-Glass"
collection: publications
permalink: /publication/sensys
excerpt: ''
date: 13.11.2023
venue: '21th ACM Conference on Embedded Networked Sensor Systems (SenSys 2023)'
paperurl: 
citation: 'Rathnayake, D., Weerakoon, D., Radhakrishnan, M., Subbaraju, V., Hwang, I. and Misra, A., 2023, November. VGGlass - Demonstrating Visual Grounding and Localization Synergy with a LiDAR-enabled Smart-Glass. In 21th ACM Conference on Embedded Networked Sensor Systems (SenSys 2023) [In Press]'
---
Demo Abstract: This work demonstrates the \names system, which simultaneously interprets human instructions for a target acquisition task and determines the precise 3D positions of both user and the target object. This is achieved by utilizing LiDARs mounted in the infrastructure and a smart glass device worn by the user. Key to our system is the union of LiDAR-based localization termed \emph{LiLOC} and a multi-modal visual grounding approach termed \emph{RealG(2)In-Lite}. To demonstrate the system, we use Intel RealSense L515 cameras and a Microsoft HoloLens 2, as the user devices. \names is able to: a) track the user in real-time in a global coordinate system, and b) locate target objects referred by natural language and pointing gestures.

<video width="320" height="240" controls>
  <source src="https://dulangaweerakoon.com/images/VGGlass_Demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<!-- Recommended citation: Weerakoon, D., Subbaraju, V., Tran, T. and Misra, A., 2022. Cosm2ic: Optimizing real-time multi-modal instruction comprehension. IEEE Robotics and Automation Letters, 7(4), pp.10697-10704. -->